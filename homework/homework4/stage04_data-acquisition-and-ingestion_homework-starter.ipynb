{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 04: Data Acquisition and Ingestion Homework\n",
    "\n",
    "This notebook demonstrates data acquisition from APIs and web scraping, with proper validation and file naming conventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import yfinance as yf\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project paths\n",
    "DATA_RAW = Path(\"data/raw\")\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load secrets\n",
    "load_dotenv()\n",
    "ALPHA_KEY = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "\n",
    "print(f\"Data directory created: {DATA_RAW}\")\n",
    "print(f\"Alpha Vantage API key available: {bool(ALPHA_KEY)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper functions\n",
    "def safe_stamp():\n",
    "    return dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def safe_filename(prefix: str, meta: Dict[str, str]) -> str:\n",
    "    mid = \"_\".join([f\"{k}-{str(v).replace(' ', '-')[:30]}\" for k,v in meta.items()])\n",
    "    return f\"{prefix}_{mid}_{safe_stamp()}.csv\"\n",
    "\n",
    "def validate_df(df: pd.DataFrame,\n",
    "                required_cols: list,\n",
    "                dtypes_map: Dict[str,str] = None,\n",
    "                min_rows: int = 1) -> dict:\n",
    "    dtypes_map = dtypes_map or {}\n",
    "    msgs = {\n",
    "        \"missing_cols\": [],\n",
    "        \"bad_dtypes\": {},\n",
    "        \"na_count\": int(df.isna().sum().sum()),\n",
    "        \"n_rows\": int(df.shape[0]),\n",
    "        \"n_cols\": int(df.shape[1]),\n",
    "    }\n",
    "    # required cols\n",
    "    msgs[\"missing_cols\"] = [c for c in required_cols if c not in df.columns]\n",
    "    # dtype checks (attempt coercion on a copy)\n",
    "    for col, t in dtypes_map.items():\n",
    "        if col not in df.columns: \n",
    "            msgs[\"bad_dtypes\"][col] = f\"missing for dtype {t}\"\n",
    "            continue\n",
    "        try:\n",
    "            if t.startswith(\"datetime\"):\n",
    "                _ = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "            elif t in (\"float\", \"float64\"):\n",
    "                _ = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            elif t in (\"int\",\"int64\"):\n",
    "                _ = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "            else:\n",
    "                _ = df[col].astype(t)\n",
    "            # if coercion yields too many NA, flag\n",
    "            if _.isna().mean() > 0.1:\n",
    "                msgs[\"bad_dtypes\"][col] = f\"too many NA after cast to {t}\"\n",
    "        except Exception as e:\n",
    "            msgs[\"bad_dtypes\"][col] = f\"cast to {t} failed: {e}\"\n",
    "    # basic shape rule\n",
    "    if msgs[\"n_rows\"] < min_rows:\n",
    "        msgs[\"bad_shape\"] = f\"rows<{min_rows}\"\n",
    "    return msgs\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: API Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOL = \"AAPL\"  # ‰Ω†ÂèØ‰ª•Êç¢Êàê‰ªªÊÑèËÇ°Á•®‰ª£Á†Å\n",
    "use_alpha = bool(ALPHA_KEY)\n",
    "\n",
    "if use_alpha:\n",
    "    url = \"https://www.alphavantage.co/query\"\n",
    "    params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY_ADJUSTED\",\n",
    "        \"symbol\": SYMBOL,\n",
    "        \"outputsize\": \"compact\",\n",
    "        \"apikey\": ALPHA_KEY,\n",
    "        \"datatype\": \"json\"\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        # ÊâæÂà∞Êó∂Èó¥Â∫èÂàóÈîÆ\n",
    "        ts_key = [k for k in js.keys() if \"Time Series\" in k]\n",
    "        assert ts_key, f\"No time series key in response: keys={list(js.keys())[:5]}\"\n",
    "        ts = js[ts_key[0]]\n",
    "        # ËΩ¨Êàê DataFrameÔºàdate, adj_closeÔºâ\n",
    "        df_api = (\n",
    "            pd.DataFrame(ts)\n",
    "              .T.reset_index()\n",
    "              .rename(columns={\"index\":\"date\", \"5. adjusted close\":\"adj_close\"})\n",
    "              [[\"date\",\"adj_close\"]]\n",
    "        )\n",
    "        # Á±ªÂûãËΩ¨Êç¢\n",
    "        df_api[\"date\"] = pd.to_datetime(df_api[\"date\"])\n",
    "        df_api[\"adj_close\"] = pd.to_numeric(df_api[\"adj_close\"], errors=\"coerce\")\n",
    "        print(f\"‚úÖ Successfully fetched data from Alpha Vantage for {SYMBOL}\")\n",
    "    except Exception as e:\n",
    "        print(\"Alpha Vantage failed, falling back to yfinance. Error:\", e)\n",
    "        use_alpha = False\n",
    "\n",
    "if not use_alpha:\n",
    "    print(f\"üìä Fetching {SYMBOL} data from yfinance...\")\n",
    "    df_api = yf.download(SYMBOL, period=\"6mo\", interval=\"1d\").reset_index()[[\"Date\",\"Adj Close\"]]\n",
    "    df_api.columns = [\"date\",\"adj_close\"]\n",
    "    df_api[\"date\"] = pd.to_datetime(df_api[\"date\"])\n",
    "    df_api[\"adj_close\"] = pd.to_numeric(df_api[\"adj_close\"], errors=\"coerce\")\n",
    "    print(f\"‚úÖ Successfully fetched data from yfinance for {SYMBOL}\")\n",
    "\n",
    "# ÊéíÂ∫è + Ê†°È™å + ËêΩÁõò\n",
    "df_api = df_api.sort_values(\"date\").reset_index(drop=True)\n",
    "msgs = validate_df(df_api,\n",
    "                   required_cols=[\"date\",\"adj_close\"],\n",
    "                   dtypes_map={\"date\":\"datetime64[ns]\",\"adj_close\":\"float\"},\n",
    "                   min_rows=10)\n",
    "print(\"API validation:\", msgs)\n",
    "\n",
    "fname = safe_filename(prefix=\"api\",\n",
    "                      meta={\"source\": \"alpha\" if use_alpha else \"yfinance\",\n",
    "                            \"symbol\": SYMBOL})\n",
    "out_path = DATA_RAW / fname\n",
    "df_api.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of API data:\")\n",
    "print(df_api.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRAPE_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"  # ÂèØÊõøÊç¢‰∏∫‰Ω†Á°ÆËÆ§ÂÖÅËÆ∏ÊäìÂèñÁöÑÈ°µÈù¢\n",
    "headers = {\"User-Agent\": \"AFE-Course-Notebook/1.0 (contact: your_email@example.com)\"}\n",
    "\n",
    "def parse_first_table(html: str) -> pd.DataFrame:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\")\n",
    "    assert table is not None, \"No <table> element found\"\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        cells = [td.get_text(strip=True) for td in tr.find_all([\"td\",\"th\"])]\n",
    "        if cells:\n",
    "            rows.append(cells)\n",
    "    header, *data = rows\n",
    "    return pd.DataFrame(data, columns=header)\n",
    "\n",
    "try:\n",
    "    print(f\"üåê Attempting to scrape: {SCRAPE_URL}\")\n",
    "    resp = requests.get(SCRAPE_URL, headers=headers, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    df_scrape = parse_first_table(resp.text)\n",
    "    print(f\"‚úÖ Successfully scraped table with {df_scrape.shape[0]} rows and {df_scrape.shape[1]} columns\")\n",
    "except Exception as e:\n",
    "    print(\"Scrape failed (using inline demo table).\", e)\n",
    "    html = \"\"\"\n",
    "    <table>\n",
    "      <tr><th>Ticker</th><th>Price</th></tr>\n",
    "      <tr><td>AAA</td><td>101.2</td></tr>\n",
    "      <tr><td>BBB</td><td>98.7</td></tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    df_scrape = parse_first_table(html)\n",
    "    print(f\"üìã Using demo table with {df_scrape.shape[0]} rows and {df_scrape.shape[1]} columns\")\n",
    "\n",
    "# Â∞ùËØïÊääÂèØËÉΩÁöÑÊï∞Â≠óÂàóËΩ¨‰∏∫Êï∞ÂÄºÔºàÁ§∫‰æãÔºöÂàóÂêçÂåÖÂê´ Price ÊàñÂ∏¶ÈÄóÂè∑/ÁæéÂÖÉÁ¨¶Âè∑Ôºâ\n",
    "for col in df_scrape.columns:\n",
    "    if any(x in col.lower() for x in [\"price\",\"close\",\"volume\",\"market\",\"cap\",\"weight\",\"%\",\"chg\",\"change\"]):\n",
    "        cleaned = (\n",
    "            df_scrape[col]\n",
    "            .str.replace(r\"[^0-9.\\-]\", \"\", regex=True)\n",
    "            .replace({\"\": None})\n",
    "        )\n",
    "        maybe_num = pd.to_numeric(cleaned, errors=\"coerce\")\n",
    "        # Âè™ÊúâÂú®Â§öÊï∞ÂÄºÂèØËΩ¨Êç¢Êó∂ÊâçÊõøÊç¢\n",
    "        if maybe_num.notna().mean() > 0.5:\n",
    "            df_scrape[col] = maybe_num\n",
    "            print(f\"üî¢ Converted column '{col}' to numeric\")\n",
    "\n",
    "msgs2 = validate_df(df_scrape, required_cols=list(df_scrape.columns), dtypes_map={}, min_rows=3)\n",
    "print(\"SCRAPE validation:\", msgs2)\n",
    "\n",
    "fname2 = safe_filename(prefix=\"scrape\", meta={\"site\":\"wikipedia\",\"table\":\"first\"})\n",
    "out_path2 = DATA_RAW / fname2\n",
    "df_scrape.to_csv(out_path2, index=False)\n",
    "print(\"Saved:\", out_path2)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of scraped data:\")\n",
    "print(df_scrape.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "print(\"üìÅ Generated files in data/raw/:\")\n",
    "for file in DATA_RAW.glob(\"*.csv\"):\n",
    "    print(f\"  - {file.name} ({file.stat().st_size} bytes)\")\n",
    "\n",
    "print(\"\\n‚úÖ Data acquisition and ingestion completed successfully!\")\n",
    "print(f\"üìä API data: {df_api.shape[0]} rows, {df_api.shape[1]} columns\")\n",
    "print(f\"üåê Scraped data: {df_scrape.shape[0]} rows, {df_scrape.shape[1]} columns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
