{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 04: Data Acquisition and Ingestion Homework\n",
    "\n",
    "This notebook demonstrates data acquisition from APIs and web scraping, with proper validation and file naming conventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os, json, datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import yfinance as yf\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory created: data/raw\n",
      "Alpha Vantage API key available: False\n"
     ]
    }
   ],
   "source": [
    "DATA_RAW = Path(\"data/raw\")\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "load_dotenv()\n",
    "ALPHA_KEY = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "\n",
    "print(f\"Data directory created: {DATA_RAW}\")\n",
    "print(f\"Alpha Vantage API key available: {bool(ALPHA_KEY)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def safe_stamp():\n",
    "    return dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def safe_filename(prefix: str, meta: Dict[str, str]) -> str:\n",
    "    mid = \"_\".join([f\"{k}-{str(v).replace(' ', '-')[:30]}\" for k,v in meta.items()])\n",
    "    return f\"{prefix}_{mid}_{safe_stamp()}.csv\"\n",
    "\n",
    "def validate_df(df: pd.DataFrame,\n",
    "                required_cols: list,\n",
    "                dtypes_map: Dict[str,str] = None,\n",
    "                min_rows: int = 1) -> dict:\n",
    "    dtypes_map = dtypes_map or {}\n",
    "    msgs = {\n",
    "        \"missing_cols\": [],\n",
    "        \"bad_dtypes\": {},\n",
    "        \"na_count\": int(df.isna().sum().sum()),\n",
    "        \"n_rows\": int(df.shape[0]),\n",
    "        \"n_cols\": int(df.shape[1]),\n",
    "    }\n",
    "    msgs[\"missing_cols\"] = [c for c in required_cols if c not in df.columns]\n",
    "    for col, t in dtypes_map.items():\n",
    "        if col not in df.columns: \n",
    "            msgs[\"bad_dtypes\"][col] = f\"missing for dtype {t}\"\n",
    "            continue\n",
    "        try:\n",
    "            if t.startswith(\"datetime\"):\n",
    "                _ = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "            elif t in (\"float\", \"float64\"):\n",
    "                _ = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            elif t in (\"int\",\"int64\"):\n",
    "                _ = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "            else:\n",
    "                _ = df[col].astype(t)\n",
    "            if _.isna().mean() > 0.1:\n",
    "                msgs[\"bad_dtypes\"][col] = f\"too many NA after cast to {t}\"\n",
    "        except Exception as e:\n",
    "            msgs[\"bad_dtypes\"][col] = f\"cast to {t} failed: {e}\"\n",
    "    if msgs[\"n_rows\"] < min_rows:\n",
    "        msgs[\"bad_shape\"] = f\"rows<{min_rows}\"\n",
    "    return msgs\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: API Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Fetching AAPL data from yfinance...\n",
      "Available columns: [('Close', 'AAPL'), ('High', 'AAPL'), ('Low', 'AAPL'), ('Open', 'AAPL'), ('Volume', 'AAPL')]\n",
      "‚úÖ Successfully fetched data from yfinance for AAPL\n",
      "API validation: {'missing_cols': [], 'bad_dtypes': {}, 'na_count': 0, 'n_rows': 125, 'n_cols': 2}\n",
      "Saved: data/raw/api_source-yfinance_ticker-AAPL_20250818-031601.csv\n",
      "\n",
      "First 5 rows of API data:\n",
      "        date   adj_close\n",
      "0 2025-02-18  243.873062\n",
      "1 2025-02-19  244.272079\n",
      "2 2025-02-20  245.229736\n",
      "3 2025-02-21  244.950424\n",
      "4 2025-02-24  246.496643\n"
     ]
    }
   ],
   "source": [
    "SYMBOL = \"AAPL\"\n",
    "use_alpha = bool(ALPHA_KEY)\n",
    "\n",
    "if use_alpha:\n",
    "    url = \"https://www.alphavantage.co/query\"\n",
    "    params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY_ADJUSTED\",\n",
    "        \"symbol\": SYMBOL,\n",
    "        \"outputsize\": \"compact\",\n",
    "        \"apikey\": ALPHA_KEY,\n",
    "        \"datatype\": \"json\"\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        ts_key = [k for k in js.keys() if \"Time Series\" in k]\n",
    "        assert ts_key, f\"No time series key in response: keys={list(js.keys())[:5]}\"\n",
    "        ts = js[ts_key[0]]\n",
    "        df_api = (\n",
    "            pd.DataFrame(ts)\n",
    "              .T.reset_index()\n",
    "              .rename(columns={\"index\":\"date\", \"5. adjusted close\":\"adj_close\"})\n",
    "              [[\"date\",\"adj_close\"]]\n",
    "        )\n",
    "        df_api[\"date\"] = pd.to_datetime(df_api[\"date\"])\n",
    "        df_api[\"adj_close\"] = pd.to_numeric(df_api[\"adj_close\"], errors=\"coerce\")\n",
    "        print(f\"‚úÖ Successfully fetched data from Alpha Vantage for {SYMBOL}\")\n",
    "    except Exception as e:\n",
    "        print(\"Alpha Vantage failed, falling back to yfinance. Error:\", e)\n",
    "        use_alpha = False\n",
    "\n",
    "if not use_alpha:\n",
    "    print(f\"üìä Fetching {SYMBOL} data from yfinance...\")\n",
    "    stock_data = yf.download(SYMBOL, period=\"6mo\", interval=\"1d\", auto_adjust=True)\n",
    "    print(\"Available columns:\", list(stock_data.columns))\n",
    "    \n",
    "    if isinstance(stock_data.columns, pd.MultiIndex):\n",
    "        stock_data.columns = [col[0] for col in stock_data.columns]\n",
    "    \n",
    "    df_api = stock_data.reset_index()[[\"Date\",\"Close\"]]\n",
    "    df_api.columns = [\"date\",\"adj_close\"]\n",
    "    df_api[\"date\"] = pd.to_datetime(df_api[\"date\"])\n",
    "    df_api[\"adj_close\"] = pd.to_numeric(df_api[\"adj_close\"], errors=\"coerce\")\n",
    "    print(f\"‚úÖ Successfully fetched data from yfinance for {SYMBOL}\")\n",
    "\n",
    "df_api = df_api.sort_values(\"date\").reset_index(drop=True)\n",
    "msgs = validate_df(df_api,\n",
    "                   required_cols=[\"date\",\"adj_close\"],\n",
    "                   dtypes_map={\"date\":\"datetime64[ns]\",\"adj_close\":\"float\"},\n",
    "                   min_rows=10)\n",
    "print(\"API validation:\", msgs)\n",
    "\n",
    "fname = safe_filename(prefix=\"api\",\n",
    "                      meta={\"source\": \"alpha\" if use_alpha else \"yfinance\",\n",
    "                            \"ticker\": SYMBOL})\n",
    "out_path = DATA_RAW / fname\n",
    "df_api.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "print(\"\\nFirst 5 rows of API data:\")\n",
    "print(df_api.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Attempting to scrape: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n",
      "‚úÖ Successfully scraped table with 503 rows and 9 columns\n",
      "SCRAPE validation: {'missing_cols': [], 'bad_dtypes': {}, 'na_count': 0, 'n_rows': 503, 'n_cols': 9}\n",
      "Saved: data/raw/scrape_site-wikipedia_table-sp500_20250818-031602.csv\n",
      "\n",
      "First 5 rows of scraped data:\n",
      "  Symbol                    Security  GICS Sector GICS Sub-Industry  \\\n",
      "0    MMM                          3M  Industrials   Industrial Conglomerates   \n",
      "1    AOS           A. O. Smith Corp.  Industrials        Building Products   \n",
      "2    ABT             Abbott Laboratories   Health Care     Health Care Equipment   \n",
      "3   ABBV                    AbbVie Inc.   Health Care           Biotechnology   \n",
      "4   ABMD                Abiomed Inc   Health Care     Health Care Equipment   \n",
      "\n",
      "  Headquarters Location Date added CIK      Founded  \n",
      "0     Saint Paul, Minnesota  1976-08-09   66740         1902  \n",
      "1       Milwaukee, Wisconsin  2017-07-26   91142         1916  \n",
      "2     North Chicago, Illinois  1964-03-31    1800         1888  \n",
      "3     North Chicago, Illinois  2012-12-31  1551152         2013  \n",
      "4        Danvers, Massachusetts  2018-05-31  815094         1981  \n"
     ]
    }
   ],
   "source": [
    "SCRAPE_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "headers = {\"User-Agent\": \"AFE-Course-Notebook/1.0 (contact: your_email@example.com)\"}\n",
    "\n",
    "def parse_first_table(html: str) -> pd.DataFrame:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\")\n",
    "    assert table is not None, \"No <table> element found\"\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        cells = [td.get_text(strip=True) for td in tr.find_all([\"td\",\"th\"])]\n",
    "        if cells:\n",
    "            rows.append(cells)\n",
    "    header, *data = rows\n",
    "    return pd.DataFrame(data, columns=header)\n",
    "\n",
    "try:\n",
    "    print(f\"üåê Attempting to scrape: {SCRAPE_URL}\")\n",
    "    resp = requests.get(SCRAPE_URL, headers=headers, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    df_scrape = parse_first_table(resp.text)\n",
    "    print(f\"‚úÖ Successfully scraped table with {df_scrape.shape[0]} rows and {df_scrape.shape[1]} columns\")\n",
    "except Exception as e:\n",
    "    print(\"Scrape failed (using inline demo table).\", e)\n",
    "    html = \"\"\"\n",
    "    <table>\n",
    "      <tr><th>Ticker</th><th>Company</th><th>Price</th><th>Market Cap</th></tr>\n",
    "      <tr><td>AAPL</td><td>Apple Inc.</td><td>150.25</td><td>2.5T</td></tr>\n",
    "      <tr><td>MSFT</td><td>Microsoft Corp</td><td>280.50</td><td>2.1T</td></tr>\n",
    "      <tr><td>GOOGL</td><td>Alphabet Inc.</td><td>125.75</td><td>1.6T</td></tr>\n",
    "      <tr><td>AMZN</td><td>Amazon.com Inc</td><td>95.30</td><td>980B</td></tr>\n",
    "      <tr><td>TSLA</td><td>Tesla Inc</td><td>220.15</td><td>700B</td></tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    df_scrape = parse_first_table(html)\n",
    "    print(f\"üìã Using demo table with {df_scrape.shape[0]} rows and {df_scrape.shape[1]} columns\")\n",
    "\n",
    "for col in df_scrape.columns:\n",
    "    if any(x in col.lower() for x in [\"price\",\"close\",\"volume\",\"market\",\"cap\",\"weight\",\"%\",\"chg\",\"change\"]):\n",
    "        cleaned = (\n",
    "            df_scrape[col]\n",
    "            .str.replace(r\"[^0-9.\\-]\", \"\", regex=True)\n",
    "            .replace({\"\": None})\n",
    "        )\n",
    "        maybe_num = pd.to_numeric(cleaned, errors=\"coerce\")\n",
    "        if maybe_num.notna().mean() > 0.5:\n",
    "            df_scrape[col] = maybe_num\n",
    "            print(f\"üî¢ Converted column '{col}' to numeric\")\n",
    "\n",
    "msgs2 = validate_df(df_scrape, required_cols=list(df_scrape.columns), dtypes_map={}, min_rows=3)\n",
    "print(\"SCRAPE validation:\", msgs2)\n",
    "\n",
    "fname2 = safe_filename(prefix=\"scrape\", meta={\"site\":\"wikipedia\",\"table\":\"sp500\"})\n",
    "out_path2 = DATA_RAW / fname2\n",
    "df_scrape.to_csv(out_path2, index=False)\n",
    "print(\"Saved:\", out_path2)\n",
    "\n",
    "print(\"\\nFirst 5 rows of scraped data:\")\n",
    "print(df_scrape.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Generated files in data/raw/:\n",
      "  - scrape_site-wikipedia_table-sp500_20250818-031602.csv (45504 bytes)\n",
      "  - api_source-yfinance_ticker-AAPL_20250817-231151.csv (3701 bytes)\n",
      "  - scrape_site-wikipedia_table-sp500_20250817-231151.csv (179 bytes)\n",
      "  - api_source-yfinance_ticker-AAPL_20250818-031601.csv (3701 bytes)\n",
      "\n",
      "‚úÖ Data acquisition and ingestion completed successfully!\n",
      "üìä API data: 125 rows, 2 columns\n",
      "üåê Scraped data: 503 rows, 9 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"üìÅ Generated files in data/raw/:\")\n",
    "for file in DATA_RAW.glob(\"*.csv\"):\n",
    "    print(f\"  - {file.name} ({file.stat().st_size} bytes)\")\n",
    "\n",
    "print(\"\\n‚úÖ Data acquisition and ingestion completed successfully!\")\n",
    "print(f\"üìä API data: {df_api.shape[0]} rows, {df_api.shape[1]} columns\")\n",
    "print(f\"üåê Scraped data: {df_scrape.shape[0]} rows, {df_scrape.shape[1]} columns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
